{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/cvda/zhangzimu/ADT/Ego-Prediction/sample_code/..\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "BASE = os.path.join(os.getcwd(), \"..\")\n",
    "print(BASE)\n",
    "sys.path.append(BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, argparse\n",
    "from base.dataset import ADT_Dataset\n",
    "from model.simple import Simple_Eye_Gaze_MLP, Simple_Eye_Gaze_Loss, Simple_Trajectory_Loss, Simple_Trajectory_MLP\n",
    "from base.utils import load_config\n",
    "from tqdm import tqdm\n",
    "from train import validate\n",
    "from base.metrics import Average_Gaze_Angular_Error, Average_Traj_Error\n",
    "import numpy as np\n",
    "import quaternion\n",
    "import rerun as rr \n",
    "from rerun.datatypes import Quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "config = load_config(\"../configs/config.yaml\")\n",
    "model_name = config['model']\n",
    "len_per_input_seq = config['len_per_input_seq']\n",
    "len_per_output_seq = config['len_per_output_seq']\n",
    "interval = config['interval']\n",
    "frame_stride = config['frame_stride']\n",
    "hidden_dim = config['hidden_dim']\n",
    "use_gpu = config['use_gpu']\n",
    "num_workers = config['num_workers']\n",
    "device = torch.device('cpu')\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"simple_gaze_mlp\":\n",
    "    model = Simple_Eye_Gaze_MLP(input_dim=3 * len_per_input_seq // frame_stride, hidden_dim=hidden_dim, output_dim=3 * len_per_output_seq // frame_stride).to(device)\n",
    "    criterion = Simple_Eye_Gaze_Loss()\n",
    "    validation_criterion = Average_Gaze_Angular_Error()\n",
    "elif model_name == \"simple_traj_mlp\":\n",
    "    model = Simple_Trajectory_MLP(input_dim=3 * len_per_input_seq // frame_stride + 16 * len_per_input_seq // frame_stride, hidden_dim=hidden_dim, output_dim=3 * len_per_output_seq // frame_stride + 4 * len_per_output_seq // frame_stride).to(device)   \n",
    "    criterion = Simple_Trajectory_Loss() \n",
    "    validation_criterion = Average_Traj_Error()\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Simple_Trajectory_MLP(\n",
       "  (fc1): Linear(in_features=380, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=256, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../logs/simple_traj_mlp.pth\")['model'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 2131.17it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = ADT_Dataset(\"../dataset/data.h5py\", len_per_input_seq, len_per_output_seq, interval, frame_stride, train=False, dataset_path=\"../dataset/\")\n",
    "dataset_length = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# piece_index = 1096\n",
    "piece_index = np.random.randint(0, dataset_length)\n",
    "input_clip, gt_clip = dataset[piece_index]\n",
    "input_clip_to_model = dict()\n",
    "for key, value in input_clip.items():\n",
    "    input_clip_to_model[key] = value.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    pred_coord, pred_quat = model(input_clip_to_model, device)\n",
    "    \n",
    "    pred_coord = pred_coord.squeeze()\n",
    "    pred_quat = pred_quat.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clip(clip, start_time, colors, input=True, pred_coord=None, pred_quat=None):\n",
    "    camera_color = [255, 0, 0]\n",
    "    camera_traj = []\n",
    "\n",
    "    for idx, timestamp in tqdm(enumerate(clip['timestamps']), total=clip['timestamps'].shape[0]):\n",
    "        time = (timestamp - start_time) / 1e9\n",
    "        rr.set_time_seconds(\"stable_time\", time)\n",
    "\n",
    "        bboxes_3d = clip['3d_boundingboxes_aabb'][idx].numpy()\n",
    "        bboxes_3d = np.asarray([bbox for bbox in bboxes_3d if bbox[0] != np.inf])\n",
    "        bboxes_3d_transform = clip['3d_boundingboxes_transform_scene_object_matrix'][idx].numpy()\n",
    "        bboxes_3d_transform = np.asarray([bbox_transform for bbox_transform in bboxes_3d_transform if bbox_transform[0, 0] != np.inf])\n",
    "\n",
    "        mins = bboxes_3d[:, [0, 2, 4]]\n",
    "        maxs = bboxes_3d[:, [1, 3, 5]]\n",
    "        translations = bboxes_3d_transform[:, :3, 3]\n",
    "        rotations = bboxes_3d_transform[:, :3, :3]\n",
    "        rotations = quaternion.from_rotation_matrix(rotations)\n",
    "        rotations_to_rerun = []\n",
    "        for rot in rotations:\n",
    "            rotations_to_rerun.append(Quaternion(xyzw=quaternion.as_float_array(rot)[[1, 2, 3, 0]]))\n",
    "        mins += translations\n",
    "        maxs += translations\n",
    "        half_sizes = (maxs - mins) / 2\n",
    "\n",
    "        centers = (mins + maxs) / 2\n",
    "        # centers = centers[:, [2, 0, 1]]\n",
    "        # half_sizes = half_sizes[:, [2, 0, 1]]\n",
    "\n",
    "        rr.log(\n",
    "            \"3D/3dboxes\",\n",
    "            rr.Boxes3D(\n",
    "                centers=centers,\n",
    "                half_sizes=half_sizes,\n",
    "                rotations=rotations_to_rerun,\n",
    "                colors = colors,\n",
    "                radii=np.ones(mins.shape[0]) * 0.01\n",
    "            )\n",
    "        )\n",
    "\n",
    "        scene_cam_matrix = clip['scene_cam_matrix'][idx].numpy()\n",
    "        camera_coord = scene_cam_matrix[:3, 3]\n",
    "        camera_traj.append(camera_coord)\n",
    "\n",
    "        camera_rot_quat = np.quaternion(*clip['cam_pose_quat'][idx])\n",
    "        camera_orientation = quaternion.rotate_vectors(camera_rot_quat, [0, 0, 0.2])\n",
    "\n",
    "        rr.log(\n",
    "            \"3D/gt_camera_coord\",\n",
    "            rr.Points3D(\n",
    "                positions=camera_coord,\n",
    "                radii=0.1,\n",
    "                colors=camera_color,\n",
    "            )\n",
    "        )\n",
    "        rr.log(\n",
    "            \"3D/gt_camera_orientation\",\n",
    "            rr.Arrows3D(\n",
    "                origins=camera_coord,\n",
    "                vectors=camera_orientation,\n",
    "                colors=camera_color,\n",
    "                radii=0.1\n",
    "            )\n",
    "        )\n",
    "        if pred_coord is not None:\n",
    "            rr.log(\n",
    "                \"3D/pred_camera_coord\",\n",
    "                rr.Points3D(\n",
    "                    positions=pred_coord[idx],\n",
    "                    radii=0.1,\n",
    "                )\n",
    "            )\n",
    "        if pred_quat is not None:\n",
    "            pred_camera_rot_quat = np.quaternion(*pred_quat[idx])\n",
    "            pred_camera_orientation = quaternion.rotate_vectors(pred_camera_rot_quat, [0, 0, 0.2])\n",
    "            rr.log(\n",
    "                \"3D/pred_camera_orientation\",\n",
    "                rr.Arrows3D(\n",
    "                    origins=pred_coord[idx],\n",
    "                    vectors=pred_camera_orientation,\n",
    "                    radii=0.1\n",
    "                )\n",
    "            )\n",
    "\n",
    "        rr.log(\n",
    "            \"2D/real_RGB\",\n",
    "            rr.Image(\n",
    "                data=clip['video'][idx]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    rr.set_time_seconds(\"stable_time\", 0)\n",
    "    camera_traj = np.asarray(camera_traj)\n",
    "    if input:\n",
    "        rr.log(\n",
    "            \"3D/input_camera_traj\",\n",
    "            rr.LineStrips3D(\n",
    "                [camera_traj],\n",
    "                colors=[0, 255, 0],\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        rr.log(\n",
    "            \"3D/output_camera_traj\",\n",
    "            rr.LineStrips3D(\n",
    "                [camera_traj],\n",
    "                colors=[0, 0, 255],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 24.97it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 32.00it/s]\n"
     ]
    }
   ],
   "source": [
    "rr.init(\"dataset_piece_visualization\")\n",
    "rr.set_time_seconds(\"stable_time\", 0)\n",
    "rr.log(\n",
    "    \"3D\",\n",
    "    rr.ViewCoordinates.RIGHT_HAND_Y_UP\n",
    ")\n",
    "\n",
    "start_time = input_clip['timestamps'][0]\n",
    "\n",
    "colors = np.random.randint(0, 255, (gt_clip['3d_boundingboxes_aabb'].shape[1], 3))\n",
    "\n",
    "visualize_clip(input_clip, start_time, colors, input=True)\n",
    "visualize_clip(gt_clip, start_time, colors, input=False, pred_coord=pred_coord, pred_quat=pred_quat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4aea10b3164cfb8cf2a0e46d892245",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Viewer()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rr.notebook_show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zzm_ego_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
